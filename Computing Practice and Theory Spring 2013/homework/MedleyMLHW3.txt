Kurt Medley
MLHW3
CPAT 2013

1. Compute the information gain and gain ratio for a decision tree with leaf nodes: {0,0,0,1,1,2}, {1,1,1,2}, {2,2,2}

info([5,5,3]) 	= entropy(5/13,5/13,3/13)
				= 0.530196778 + 0.530196778 + 0.48818705 = 1.548580606
				
info([0,0,0,1,1,2])	= 0.5 + 0.52832 = 1.02832
info([1,1,1,2])		= 0.5
info([2,2,2])		= 0

6/13 * 1.02832 + 4/13 * .05 + 0 = 0.6284554

Information Gain 		= 0.920125206
Information Gain Ratio	= 0.40583

2. Simplify the expression: log2(3/5) + log2(5/7) + log2(7/9)

-0.737 - 0.4854 - 0.3626 = -1.585

3. What is loge(1)?

0

4. Apply a version of the k-means algorithm (Euclidean distance) to create two clusters for the following data:
    (1, 0), (2, 0), (2.9, 0), (4, 0), (5, 0)
	
When does the algorithm terminate?

The k-means algorithm terminates when change in clusters drops below a specific threshold; which means the answer is optimal

Does the result depend on which points you choose as the seeds?

Yes the result depends on which k(s) you choose first.

What is the recommended strategy in the text?

Choose the initial seed at random from the entire space.  An arbitrary seed which has a uniform probability distribution.  Then choose the second seed with a probability that is proportional to the square of the distance from the first.  Proceed, at each stage choosing the next seed with a probability proportional to the square of the distance from the closest seed that has already been chosen.

What are the possible clusters?

Seeds (2,0) and (4,0)

[(1,0),(2,0),(2.9,0)],[(4,0),(5,0)]

find the inverse of the following matrix:
| 1  1|		| 1/2	-1/2 |
|-1  1|	= 	| 1/2	1/2	 |